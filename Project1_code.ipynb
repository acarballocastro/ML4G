{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries that are required to run your project\n",
    "# You are allowed to add more libraries as you need\n",
    "\n",
    "import pyBigWig\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.1 - Modeling Choices & Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import preprocess_cage_data, prepare_train_validation, prepare_test, create_dataset\n",
    "\n",
    "path_data= \"../label_data/\"\n",
    "path_expression = \"../CAGE-train\"\n",
    "# ---------------------------INSERT CODE HERE---------------------------\n",
    "\n",
    "if not os.path.exists(path_data):\n",
    "    os.makedirs(path_data)\n",
    "    preprocess_cage_data(path_expression)\n",
    "\n",
    "    # Prepare the data for training and validation\n",
    "    prepare_train_validation(path_data)\n",
    "\n",
    "    # Prepare the data for testing\n",
    "    prepare_test(path_data)\n",
    "\n",
    "bin_size = 250\n",
    "create_dataset(path_data, 20000, bin_size)\n",
    "\n",
    "# ---------------------------------------------------------------------- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.2 - Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "\n",
    "# ---------------------------INSERT CODE HERE---------------------------\n",
    "\n",
    "# fix random seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "# load data\n",
    "X_train = pickle.load(open(\"../data/X_train.pickle\", \"rb\"))\n",
    "X_val = pickle.load(open(\"../data/X_val.pickle\", \"rb\"))\n",
    "train = list(X_train.items())\n",
    "validation = list(X_val.items())\n",
    "\n",
    "# random shuffle train\n",
    "random.shuffle(train)\n",
    "# Prepare train tensor\n",
    "train_names = [x[0] for x in train]\n",
    "train_features = [x[1][0] for x in train]\n",
    "train_labels = [x[1][1] for x in train]\n",
    "train_features_nparray = np.stack([i for i in train_features], axis=0)\n",
    "# to torch tensor\n",
    "x_data_train = torch.from_numpy(train_features_nparray.astype('float32'))\n",
    "y_data_train = torch.from_numpy(np.array(train_labels).astype('float32'))\n",
    "# Prepare validation tensor\n",
    "val_names = [x[0] for x in validation]\n",
    "val_features = [x[1][0] for x in validation]\n",
    "val_labels = [x[1][1] for x in validation]\n",
    "val_features_nparray = np.stack([i for i in val_features], axis=0)\n",
    "x_data_val = torch.from_numpy(val_features_nparray.astype('float32'))  # batch x seq_len x dim\n",
    "y_data_val = torch.from_numpy(np.array(val_labels).astype('float32'))\n",
    "# Create a -1000 tensor and append to the first column of x_data in the dimension 1\n",
    "initial_tensor = torch.full((x_data_train.size(dim=0), 1, x_data_train.size(dim=2)), -1000)\n",
    "x_data_train = torch.cat((initial_tensor, x_data_train), dim=1)\n",
    "# check that x_data has -1000 in first position of dimension 1\n",
    "assert torch.all(x_data_train[:, 0, :] == -1000)\n",
    "x_data_train.to(device)\n",
    "y_data_train.to(device)\n",
    "# split data\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.1, random_state=42)\n",
    "# load pytorch model from file 'model.pt'\n",
    "# create model\n",
    "model = TransformerRegressor()\n",
    "# if file exists load\n",
    "#if os.path.isfile('model_transformer.pt'):\n",
    "#   model.model.load_state_dict(torch.load('model_transformer.pt'))\n",
    "model.set_validation_data(x_data_val, y_data_val)\n",
    "# fit model\n",
    "model.fit(x_data_train, y_data_train)\n",
    "# detach from gpu and save model\n",
    "model.model.cpu()\n",
    "torch.save(model.model.state_dict(), 'model_improved_100epochs_seed.pt')\n",
    "\n",
    "# ----------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.3 - Prediction on Test Data (Evaluation Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Using the model trained in WP 1.2, make predictions on the test data (chr 1 of cell line X3).\n",
    "# Store predictions in a variable called \"pred\" which is a numpy array.\n",
    "\n",
    "pred = None\n",
    "# ---------------------------INSERT CODE HERE---------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Check if \"pred\" meets the specified constrains\n",
    "assert isinstance(pred, np.ndarray), 'Prediction array must be a numpy array'\n",
    "assert np.issubdtype(pred.dtype, np.number), 'Prediction array must be numeric'\n",
    "assert pred.shape[0] == len(test_genes), 'Each gene should have a unique predicted expression'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Predictions in the Required Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store predictions in a ZIP. \n",
    "# Upload this zip on the project website under \"Your submission\".\n",
    "# Zip this notebook along with the conda environment (and README, optional) and upload this under \"Your code\".\n",
    "\n",
    "save_dir = 'path/to/save/output/file'  # TODO\n",
    "file_name = 'gex_predicted.csv'         # PLEASE DO NOT CHANGE THIS\n",
    "zip_name = \"LastName_FirstName_Project1.zip\" # TODO\n",
    "save_path = f'{save_dir}/{zip_name}'\n",
    "compression_options = dict(method=\"zip\", archive_name=file_name)\n",
    "\n",
    "test_genes['gex_predicted'] = pred.tolist()\n",
    "test_genes[['gene_name', 'gex_predicted']].to_csv(save_path, compression=compression_options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
